{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cc9efe",
   "metadata": {},
   "source": [
    "# ðŸ‘– Variational Autoencoders - Fashion-MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5cd183",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own autoencoder on the fashion MNIST dataset.\n",
    "\n",
    "The code has been adapted from the excellent VAE tutorial created by Francois Chollet, available on the Keras website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "from torchvision import datasets\n",
    "from rich.progress import Progress\n",
    "from torchvision.transforms import v2\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2101d61",
   "metadata": {},
   "source": [
    "## 0. Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 5\n",
    "BETA = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Let's use CUDA ({gpu_name})\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c2c5b",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Pad([2, 2, 2, 2], padding_mode='constant', fill=0.0),\n",
    "    ]\n",
    ")\n",
    "train_set = datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms)\n",
    "test_set = datasets.FashionMNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae719e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some items of clothing from the training set\n",
    "x_train = np.array([train_set[i][0] for i in range(10)])\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a56379",
   "metadata": {},
   "source": [
    "## 2. Build the variational autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        epsilon = torch.randn_like(z_mean)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949deb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.z_mean_head = nn.Linear(128 * 4 * 4, EMBEDDING_DIM)\n",
    "        self.z_log_var_head = nn.Linear(128 * 4 * 4, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        z_mean = self.z_mean_head(x)\n",
    "        z_log_var = self.z_log_var_head(x)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04589b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_DIM, 128 * 4 * 4),\n",
    "            nn.Unflatten(1, (128, 4, 4)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, CHANNELS, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a88d5e",
   "metadata": {},
   "source": [
    "## 3. Train the variational autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "sampler = Sampling().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "train_parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "opt = torch.optim.Adam(train_parameters, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac92f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Progress() as progress:\n",
    "    ema_total_loss = 0.0\n",
    "    ema_rec_loss = 0.0\n",
    "    ema_kl_loss = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_task = progress.add_task('Training...', total=len(train_loader))\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for X, _ in train_loader:\n",
    "            X = X.to(device)\n",
    "            z_mean, z_log_var = encoder(X)\n",
    "            z = sampler(z_mean, z_log_var)\n",
    "            X_hat = decoder(z)\n",
    "            reconstruction_loss = BETA * F.binary_cross_entropy(X_hat, X)\n",
    "            kl_loss = torch.mean(-0.5 * torch.sum(1.0 + z_log_var - z_mean**2.0 - torch.exp(z_log_var), dim=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "            opt.zero_grad()\n",
    "            total_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            ema_total_loss = 0.9 * ema_total_loss + 0.1 * total_loss.item()\n",
    "            ema_rec_loss = 0.9 * ema_rec_loss + 0.1 * reconstruction_loss.item()\n",
    "            ema_kl_loss = 0.9 * ema_kl_loss + 0.1 * kl_loss.item()\n",
    "            description = f'[{epoch + 1}/{EPOCHS}] loss: {ema_total_loss:.4f}, rec_loss: {ema_rec_loss:.4f}, kl_loss: {ema_kl_loss:.4f}'\n",
    "\n",
    "            progress.update(train_task, advance=1, description=description)\n",
    "\n",
    "    progress.update(train_task, refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./models', exist_ok=True)\n",
    "ckpt_dict = {\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "    'opt': opt.state_dict(),\n",
    "}\n",
    "torch.save(ckpt_dict, './models/vae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c872e3",
   "metadata": {},
   "source": [
    "## 3. Reconstruction using the variational autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the test set\n",
    "n_to_predict = 5000\n",
    "test_subset = torch.utils.data.Subset(test_set, range(n_to_predict))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder predictions and display\n",
    "z = []\n",
    "z_mean = []\n",
    "z_log_var = []\n",
    "example_images = []\n",
    "example_labels = []\n",
    "reconstructions = []\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "for X, y in test_loader:\n",
    "    example_images.append(X.numpy())\n",
    "    example_labels.append(y.numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "        mu, log_var = encoder(X)\n",
    "        z_mean.append(mu.detach().cpu().numpy())\n",
    "        z_log_var.append(log_var.detach().cpu().numpy())\n",
    "\n",
    "        sample = sampler(mu, log_var)\n",
    "        z.append(sample.detach().cpu().numpy())\n",
    "\n",
    "        X_hat = decoder(sample)\n",
    "        reconstructions.append(X_hat.detach().cpu().numpy())\n",
    "\n",
    "z = np.concatenate(z, axis=0)\n",
    "z_mean = np.concatenate(z_mean, axis=0)\n",
    "z_log_var = np.concatenate(z_log_var, axis=0)\n",
    "example_images = np.concatenate(example_images, axis=0)\n",
    "example_labels = np.concatenate(example_labels, axis=0)\n",
    "reconstructions = np.concatenate(reconstructions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc666123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example real clothing items\")\n",
    "display(example_images)\n",
    "print(\"Reconstructions\")\n",
    "display(reconstructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d44dd",
   "metadata": {},
   "source": [
    "## 4. Embed using the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples of the embeddings\n",
    "print(z[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b027b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the encoded points in 2D space\n",
    "figsize = 8\n",
    "\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3fd5c",
   "metadata": {},
   "source": [
    "## 5. Generate using the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some points in the latent space, from the standard normal distribution\n",
    "grid_width, grid_height = (6, 3)\n",
    "z_sample = np.random.normal(size=(grid_width * grid_height, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e675344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sampled points\n",
    "reconstructions = []\n",
    "for i in range(0, len(z_sample), BATCH_SIZE):\n",
    "    i0 = i\n",
    "    i1 = min(i + BATCH_SIZE, len(z_sample))\n",
    "    z_batch = torch.Tensor(z_sample[i0:i1]).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        X_hat = decoder(z_batch)\n",
    "        reconstructions.append(X_hat.detach().cpu().numpy())\n",
    "\n",
    "reconstructions = np.concatenate(reconstructions, axis=0)\n",
    "reconstructions = np.squeeze(reconstructions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original embeddings and sampled embeddings to p-values\n",
    "p = norm.cdf(z)\n",
    "p_sample = norm.cdf(z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of...\n",
    "figsize = 8\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "# ... the original embeddings ...\n",
    "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n",
    "\n",
    "# ... and the newly generated points in the latent space\n",
    "plt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\n",
    "plt.show()\n",
    "\n",
    "# Add underneath a grid of the decoded images\n",
    "fig = plt.figure(figsize=(figsize, grid_height * 2))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.35,\n",
    "        str(np.round(z_sample[i, :], 1)),\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f428f2e",
   "metadata": {},
   "source": [
    "## 6. Explore the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30598610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 8\n",
    "fig = plt.figure(figsize=(figsize * 2.5, figsize))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plot_1 = ax.scatter(z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(plot_1)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plot_2 = ax.scatter(p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(plot_2)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831cc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 12\n",
    "grid_size = 15\n",
    "fig, ax = plt.subplots(figsize=(figsize, figsize))\n",
    "sc = ax.scatter(\n",
    "    p[:, 0],\n",
    "    p[:, 1],\n",
    "    cmap=\"rainbow\",\n",
    "    c=example_labels,\n",
    "    alpha=0.8,\n",
    "    s=30,\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Add a colorbar in a new axis beside the main plot\n",
    "cax = make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(sc, cax=cax)\n",
    "\n",
    "# Show the reconstructions for grid points in the latent space\n",
    "x = norm.ppf(np.linspace(0, 1, grid_size))\n",
    "y = norm.ppf(np.linspace(1, 0, grid_size))\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "xv = xv.flatten()\n",
    "yv = yv.flatten()\n",
    "grid = np.array(list(zip(xv, yv)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_torch = torch.tensor(grid, dtype=torch.float32, device=device)\n",
    "    reconstructions = decoder(grid_torch)\n",
    "\n",
    "alpha_channel = reconstructions.clone()\n",
    "color_channels = 1.0 - reconstructions.repeat(1, 3, 1, 1)\n",
    "reconstructions = torch.cat([color_channels, alpha_channel], dim=1)\n",
    "reconstructions = reconstructions.permute(0, 2, 3, 1)\n",
    "reconstructions = reconstructions.cpu().detach().numpy()\n",
    "\n",
    "# Create a grid of locations where the reconstructions will be shown\n",
    "xs = np.linspace(0, 1, grid_size)\n",
    "ys = np.linspace(1, 0, grid_size)\n",
    "dx = xs[1] - xs[0]\n",
    "dy = ys[0] - ys[1]\n",
    "xx, yy = np.meshgrid(xs, ys, indexing='xy')\n",
    "grid = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "\n",
    "# fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(grid_size**2):\n",
    "    cx, cy = grid[i]\n",
    "    ax.imshow(\n",
    "        reconstructions[i].squeeze(),\n",
    "        extent=[cx - dx / 2, cx + dx / 2, cy - dy / 2, cy + dy / 2],\n",
    "        origin=\"upper\",\n",
    "        zorder=2,\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "ax.set_xlim(xs[0] - dx / 2, xs[-1] + dx / 2)\n",
    "ax.set_ylim(ys[-1] - dy / 2, ys[0] + dy / 2)\n",
    "ax.set_aspect('equal')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c039a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gendl-83qhsqs3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

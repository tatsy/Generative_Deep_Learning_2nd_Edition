{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab0e3d6",
   "metadata": {},
   "source": [
    "# ðŸ‘– Autoencoders on Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3889a",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own autoencoder on the fashion MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from rich.progress import Progress\n",
    "from torchvision.transforms import v2\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a02665",
   "metadata": {},
   "source": [
    "## 0. Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b67d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Let's use CUDA ({gpu_name})\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052bc0c",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d676dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Pad([2, 2, 2, 2], padding_mode='constant', fill=0.0),\n",
    "    ]\n",
    ")\n",
    "train_set = datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms)\n",
    "test_set = datasets.FashionMNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb598ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some items of clothing from the training set\n",
    "x_train = np.array([train_set[i][0] for i in range(10)])\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979dc66",
   "metadata": {},
   "source": [
    "## 2. Build the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, EMBEDDING_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae87cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_DIM, 128 * 4 * 4),\n",
    "            nn.Unflatten(1, (128, 4, 4)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, CHANNELS, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "train_parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "opt = torch.optim.Adam(train_parameters, lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9225c5",
   "metadata": {},
   "source": [
    "## 3. Train the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Progress() as progress:\n",
    "    ema_loss = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_task = progress.add_task('Training...', total=len(train_loader))\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for X, _ in train_loader:\n",
    "            X = X.to(device)\n",
    "            z = encoder(X)\n",
    "            X_hat = decoder(z)\n",
    "            loss = F.binary_cross_entropy(X_hat, X)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            ema_loss = 0.9 * ema_loss + 0.1 * loss.item() if ema_loss else loss.item()\n",
    "            progress.update(train_task, advance=1, description=f'[{epoch + 1}/{EPOCHS}] loss: {ema_loss:.4f}')\n",
    "\n",
    "    progress.update(train_task, refresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9eb37d",
   "metadata": {},
   "source": [
    "## 4. Reconstruct using the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335accfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 5000\n",
    "test_subset = torch.utils.data.Subset(test_set, np.arange(n_to_predict))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d146a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0.0\n",
    "count = 0\n",
    "example_images = []\n",
    "example_labels = []\n",
    "predictions = []\n",
    "\n",
    "with Progress() as progress:\n",
    "    test_task = progress.add_task('Testing...', total=len(test_loader))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for X, y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            z = encoder(X)\n",
    "            X_hat = decoder(z)\n",
    "\n",
    "        loss = F.binary_cross_entropy(X_hat, X)\n",
    "        avg_loss += loss.item() * X.size(0)\n",
    "        count += X.size(0)\n",
    "\n",
    "        example_images.append(X.cpu().detach().numpy())\n",
    "        example_labels.append(y.cpu().detach().numpy())\n",
    "        predictions.append(X_hat.cpu().detach().numpy())\n",
    "\n",
    "        progress.update(test_task, advance=1)\n",
    "\n",
    "    avg_loss /= count\n",
    "    example_images = np.concatenate(example_images, axis=0)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    progress.update(test_task, refresh=True)\n",
    "    progress.console.log(f'[Test] loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a482a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example real clothing items\")\n",
    "display(example_images)\n",
    "print(\"Reconstructions\")\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59f82a",
   "metadata": {},
   "source": [
    "## 5. Embed using the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the example images\n",
    "embeddings = []\n",
    "\n",
    "with Progress() as progress:\n",
    "    test_task = progress.add_task('Testing...', total=len(test_loader))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for X, _ in test_loader:\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            z = encoder(X)\n",
    "\n",
    "        embeddings.append(z.cpu().detach().numpy())\n",
    "        progress.update(test_task, advance=1)\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    progress.update(test_task, refresh=True)\n",
    "    progress.console.log(f'[Test] loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples of the embeddings\n",
    "print(embeddings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the encoded points in 2D space\n",
    "figsize = 8\n",
    "\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c='black', alpha=0.5, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a55807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (colothing type - see table)\n",
    "figsize = 8\n",
    "\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(\n",
    "    embeddings[:, 0],\n",
    "    embeddings[:, 1],\n",
    "    cmap='rainbow',\n",
    "    c=example_labels[:n_to_predict],\n",
    "    alpha=0.8,\n",
    "    s=3,\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc04491",
   "metadata": {},
   "source": [
    "## 6. Generate using the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range of the existing embeddings\n",
    "mins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)\n",
    "\n",
    "# Sample some points in the latent space\n",
    "grid_width, grid_height = (6, 3)\n",
    "sample = np.random.uniform(mins, maxs, size=(grid_width * grid_height, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sampled points\n",
    "sample_torch = torch.tensor(sample, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = decoder(sample_torch)\n",
    "\n",
    "reconstructions = reconstructions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff10514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of...\n",
    "figsize = 8\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "# ... the original embeddings ...\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c='black', alpha=0.5, s=2)\n",
    "\n",
    "# ... and the newly generated points in the latent space\n",
    "plt.scatter(sample[:, 0], sample[:, 1], c='#00B0F0', alpha=1, s=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add underneath a grid of the decoded images\n",
    "fig = plt.figure(figsize=(figsize, grid_height * 2))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.imshow(reconstructions[i].squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.35,\n",
    "        str(np.round(sample[i, :], 1)),\n",
    "        fontsize=10,\n",
    "        ha='center',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.imshow(reconstructions[i].squeeze(), cmap='Greys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 12\n",
    "grid_size = 15\n",
    "fig, ax = plt.subplots(figsize=(figsize, figsize))\n",
    "sc = ax.scatter(\n",
    "    embeddings[:, 0],\n",
    "    embeddings[:, 1],\n",
    "    cmap='rainbow',\n",
    "    c=example_labels,\n",
    "    alpha=0.8,\n",
    "    s=30,\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Add a colorbar in a new axis beside the main plot\n",
    "cax = make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(sc, cax=cax)\n",
    "\n",
    "# Show the reconstructions for grid points in the latent space\n",
    "x = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\n",
    "y = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "xv = xv.flatten()\n",
    "yv = yv.flatten()\n",
    "grid = np.array(list(zip(xv, yv)))\n",
    "with torch.no_grad():\n",
    "    grid_torch = torch.tensor(grid, dtype=torch.float32, device=device)\n",
    "    reconstructions = decoder(grid_torch)\n",
    "\n",
    "alpha_channel = reconstructions.clone()\n",
    "color_channels = 1.0 - reconstructions.repeat(1, 3, 1, 1)\n",
    "reconstructions = torch.cat([color_channels, alpha_channel], dim=1)\n",
    "reconstructions = reconstructions.permute(0, 2, 3, 1)\n",
    "reconstructions = reconstructions.cpu().detach().numpy()\n",
    "\n",
    "# Create a grid of locations where the reconstructions will be shown\n",
    "xs = np.linspace(embeddings[:, 0].min(), embeddings[:, 0].max(), grid_size)\n",
    "ys = np.linspace(embeddings[:, 1].max(), embeddings[:, 1].min(), grid_size)\n",
    "dx = xs[1] - xs[0]\n",
    "dy = ys[0] - ys[1]\n",
    "xx, yy = np.meshgrid(xs, ys, indexing='xy')\n",
    "grid = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "\n",
    "# fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(grid_size**2):\n",
    "    cx, cy = grid[i]\n",
    "    ax.imshow(\n",
    "        reconstructions[i].squeeze(),\n",
    "        extent=[cx - dx / 2, cx + dx / 2, cy - dy / 2, cy + dy / 2],\n",
    "        origin=\"upper\",\n",
    "        zorder=2,\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "ax.set_xlim(xs[0] - dx / 2, xs[-1] + dx / 2)\n",
    "ax.set_ylim(ys[-1] - dy / 2, ys[0] + dy / 2)\n",
    "ax.set_aspect('equal')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33432577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gendl-83qhsqs3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

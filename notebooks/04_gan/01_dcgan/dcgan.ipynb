{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a076cfa2",
   "metadata": {},
   "source": [
    "# ðŸ§± DCGAN - Bricks Data\n",
    "\n",
    "In this notebook, we'll walk through the steps required to train your own DCGAN on the bricks dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from rich.progress import Progress\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab70cc",
   "metadata": {},
   "source": [
    "## 0. Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "EPOCHS = 300\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.999\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Let's use CUDA ({gpu_name})\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd417405",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegoBrickDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    LEGO brick image dataset (from Kaggle)\n",
    "    URL: https://www.kaggle.com/datasets/joosthazelzet/lego-brick-images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root='', transform=None):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.image_files = sorted(list(self.data_root.rglob('*.png')))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.image_files[index]\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        v2.Grayscale(num_output_channels=CHANNELS),\n",
    "        v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = LegoBrickDataset(\n",
    "    data_root='./data/lego-brick-images/dataset',\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949aad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = np.array([train_data[i] for i in range(10)])\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c4436",
   "metadata": {},
   "source": [
    "## 2. Build the GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        layer5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.net = nn.ModuleList([layer1, layer2, layer3, layer4, layer5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "\n",
    "        return torch.sigmoid(x.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e04829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(Z_DIM, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        layer5 = nn.ConvTranspose2d(64, CHANNELS, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "\n",
    "        self.net = nn.ModuleList([layer1, layer2, layer3, layer4, layer5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, Z_DIM, 1, 1)\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n",
    "\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA_1, ADAM_BETA_2))\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA_1, ADAM_BETA_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5c114",
   "metadata": {},
   "source": [
    "## 3. Train the GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.train()\n",
    "netG.train()\n",
    "\n",
    "ema_d_loss = 0.0\n",
    "ema_d_acc = 0.0\n",
    "ema_g_loss = 0.0\n",
    "ema_g_acc = 0.0\n",
    "\n",
    "with Progress() as progress:\n",
    "    train_task = progress.add_task('Training', total=len(train_dataloader) * EPOCHS)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for real_images in train_dataloader:\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            # Sample random points in the latent space\n",
    "            batch_size = real_images.size(0)\n",
    "            random_latent_vectors = torch.randn((batch_size, Z_DIM)).to(device)\n",
    "\n",
    "            # Train the discriminator on real/fake images\n",
    "            generated_images = netG(random_latent_vectors)\n",
    "            real_predictions = netD(real_images)\n",
    "            fake_predictions = netD(generated_images.detach())  # <- IMPORTANT: detach required\n",
    "\n",
    "            # Label smoothing\n",
    "            real_labels = torch.ones_like(real_predictions)\n",
    "            real_noisy_labels = real_labels - NOISE_PARAM * torch.rand_like(real_labels)\n",
    "            fake_labels = torch.zeros_like(fake_predictions)\n",
    "            fake_noisy_labels = fake_labels + NOISE_PARAM * torch.rand_like(fake_labels)\n",
    "\n",
    "            d_real_loss = F.binary_cross_entropy(real_predictions, real_noisy_labels)\n",
    "            d_fake_loss = F.binary_cross_entropy(fake_predictions, fake_noisy_labels)\n",
    "            d_loss = (d_real_loss + d_fake_loss) * 0.5\n",
    "\n",
    "            optimD.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimD.step()\n",
    "\n",
    "            # Train the generator on fake images\n",
    "            random_latent_vectors = torch.randn((batch_size, Z_DIM)).to(device)\n",
    "            generated_images = netG(random_latent_vectors)\n",
    "            fake_predictions = netD(generated_images)\n",
    "\n",
    "            g_loss = F.binary_cross_entropy(fake_predictions, real_labels)\n",
    "            optimG.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimG.step()\n",
    "\n",
    "            # Update metrics\n",
    "            d_real_acc = (real_predictions > 0.5).float().mean()\n",
    "            d_fake_acc = (fake_predictions < 0.5).float().mean()\n",
    "            d_acc = (d_real_acc + d_fake_acc) * 0.5\n",
    "            g_acc = (fake_predictions > 0.5).float().mean()\n",
    "\n",
    "            ema_d_loss = 0.9 * ema_d_loss + 0.1 * d_loss.item()\n",
    "            ema_d_acc = 0.9 * ema_d_acc + 0.1 * d_acc.item()\n",
    "            ema_g_loss = 0.9 * ema_g_loss + 0.1 * g_loss.item()\n",
    "            ema_g_acc = 0.9 * ema_g_acc + 0.1 * g_acc.item()\n",
    "\n",
    "            # Display metrics\n",
    "            metrics = (\n",
    "                f'd-loss: {ema_d_loss:.4f} | '\n",
    "                f'd-acc: {ema_d_acc:.4f} | '\n",
    "                f'g-loss: {ema_g_loss:.4f} | '\n",
    "                f'g-acc: {ema_g_acc:.4f}'\n",
    "            )\n",
    "            progress.update(train_task, advance=1, description=f'Epoch {epoch + 1}/{EPOCHS} | {metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoints\n",
    "checkpoint_dir = Path('./checkpoint')\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_dict = {\n",
    "    'netD': netD.state_dict(),\n",
    "    'netG': netG.state_dict(),\n",
    "}\n",
    "torch.save(ckpt_dict, checkpoint_dir / 'checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd69d7",
   "metadata": {},
   "source": [
    "## 3. Generate new images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some points in the latent space, from the standard normal distribution\n",
    "grid_width, grid_height = (10, 3)\n",
    "z_sample = torch.randn((grid_width * grid_height, Z_DIM)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = netG(z_sample).cpu().numpy()\n",
    "    reconstructions = reconstructions * 0.5 + 0.5\n",
    "    reconstructions = reconstructions.reshape((-1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5135b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(img1, img2):\n",
    "    return np.mean(np.abs(img1 - img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in range(len(train_data)):\n",
    "    img = train_data[i].reshape((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img = img * 0.5 + 0.5\n",
    "    all_data.append(img)\n",
    "all_data = np.stack(all_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 3, 5\n",
    "fig, axs = plt.subplots(r, c, figsize=(10, 6))\n",
    "fig.suptitle(\"Generated images\", fontsize=20)\n",
    "\n",
    "noise = torch.randn(r * c, Z_DIM).to(device)\n",
    "with torch.no_grad():\n",
    "    gen_imgs = netG(noise).cpu().numpy()\n",
    "\n",
    "gen_imgs = (gen_imgs * 0.5 + 0.5).reshape((-1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f36c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(r, c, figsize=(10, 6))\n",
    "fig.suptitle(\"Closest images in the training set\", fontsize=20)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        c_diff = 99999\n",
    "        c_img = None\n",
    "        for k_idx, k in enumerate(all_data):\n",
    "            diff = compare_images(gen_imgs[cnt], k)\n",
    "            if diff < c_diff:\n",
    "                c_img = np.copy(k)\n",
    "                c_diff = diff\n",
    "        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde950b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gendl-83qhsqs3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
